# TraceLLM

This repository provides am expansion of the watermarking technique for LLMs described in the paper "[A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)" by Kirchenbauer, Geiping, Wen, Katz, Miers, and Raskar.  It allows you to nearly invisbly embed a watermark into text generated by your LLM, enabling detection of whether a given text snippet was likely produced by your specific model.  ​​This is accomplished through probabilistic tuning of the output tokens via hashing of predecessors, causing minimal effects to the quality of the output while also being able to pick up the water from just a few sentences.

**Key Features:**

*   **Watermark Embedding:**  Integrates seamlessly with your existing text generation pipeline.  It works by biasing the probability distribution over the vocabulary, encouraging the selection of tokens from a dynamically generated "green list."
*   **Watermark Detection:**  Provides a statistically rigorous method to determine if a given text sample likely contains the watermark.  This is based on analyzing the proportion of "green list" tokens present.
*   **Adjustable Parameters:**  Allows you to fine-tune the strength of the watermark (gamma), the size of the green list (delta), and a hashing function used for randomization.  This lets you control the trade-off between watermark robustness and potential impact on text fluency.
*   **Easy Integration:** Designed for easy integration with popular LLM frameworks (e.g., Hugging Face Transformers, PyTorch).  Examples and documentation are provided.
